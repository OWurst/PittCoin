{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8002685,"sourceType":"datasetVersion","datasetId":4712805},{"sourceId":8023344,"sourceType":"datasetVersion","datasetId":4728116},{"sourceId":8096544,"sourceType":"datasetVersion","datasetId":4780597}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bert Credibility Model Training and Testing\nThis notebook shows the process to train a bert model to predict the credibility of news data as well as the evaluation of the model.","metadata":{}},{"cell_type":"markdown","source":"### Requirement Installation and Data Preparation\nIn this first section we will install our dependencies, import the necessary packages, and prepare our data for training tasks. Our dependencies can be found in the requirements.txt file, but we are primarily using pandas for data manipulation, the HuggingFace transformers library for training, and sklearn for model evaluation. Our data is in 3 csv files that have already been split into train/dev/test. The data in our dataset has 3 columns: an article's title, its text, and a binary credibility label with 1 being credible and 0 being fake.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"#### Installs and Imports ","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirements/requirements.txt\n!pip install scikit-learn","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-12T00:50:46.764714Z","iopub.execute_input":"2024-04-12T00:50:46.765284Z","iopub.status.idle":"2024-04-12T00:51:19.806959Z","shell.execute_reply.started":"2024-04-12T00:50:46.765244Z","shell.execute_reply":"2024-04-12T00:51:19.805606Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 1)) (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 2)) (1.26.4)\nCollecting langdetect (from -r /kaggle/input/requirements/requirements.txt (line 3))\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 4)) (0.28.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 5)) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 6)) (4.39.3)\nCollecting seqeval (from -r /kaggle/input/requirements/requirements.txt (line 7))\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 8)) (2.18.0)\nCollecting evaluate (from -r /kaggle/input/requirements/requirements.txt (line 9))\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/input/requirements/requirements.txt (line 10)) (1.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r /kaggle/input/requirements/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r /kaggle/input/requirements/requirements.txt (line 1)) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r /kaggle/input/requirements/requirements.txt (line 1)) (2023.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect->-r /kaggle/input/requirements/requirements.txt (line 3)) (1.16.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (0.15.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (3.9.1)\nCollecting responses<0.19 (from evaluate->-r /kaggle/input/requirements/requirements.txt (line 9))\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r /kaggle/input/requirements/requirements.txt (line 10)) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r /kaggle/input/requirements/requirements.txt (line 10)) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r /kaggle/input/requirements/requirements.txt (line 10)) (3.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->-r /kaggle/input/requirements/requirements.txt (line 8)) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate->-r /kaggle/input/requirements/requirements.txt (line 4)) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r /kaggle/input/requirements/requirements.txt (line 6)) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r /kaggle/input/requirements/requirements.txt (line 5)) (1.3.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nBuilding wheels for collected packages: langdetect, seqeval\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=b50e816183cdadc7bb38debefc5c6c6ef4edd63ada54b69bb54ca28d7a109a6a\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=d0b4fd6d6548aca1d3c6a9308ed4b12d65a9fb6ad204a7c9c6b37cebd27b6242\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built langdetect seqeval\nInstalling collected packages: langdetect, responses, seqeval, evaluate\nSuccessfully installed evaluate-0.4.1 langdetect-1.0.9 responses-0.18.0 seqeval-1.2.2\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer\nfrom transformers import Trainer, TrainingArguments\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import functional as F\nfrom torch.nn.functional import softmax\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport os\nimport shutil\nimport zipfile\n\nfrom IPython.display import HTML\nfrom IPython.display import FileLink","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-12T15:01:22.768312Z","iopub.execute_input":"2024-04-12T15:01:22.769150Z","iopub.status.idle":"2024-04-12T15:01:41.664608Z","shell.execute_reply.started":"2024-04-12T15:01:22.769112Z","shell.execute_reply":"2024-04-12T15:01:41.663599Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-04-12 15:01:33.021942: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-12 15:01:33.022055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-12 15:01:33.158123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Data Preparation:\nAfter the data is read in, it still is not ready for BERT to train. First, we need to concatenate the title and text with the proper separator token, and then we need to build a custom dataset object to properly set up the data for bert","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/credibility-data/full_data_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/credibility-data/full_data_test.csv\")\ndev_data = pd.read_csv(\"/kaggle/input/credibility-data/full_data_dev.csv\")\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:01:48.093895Z","iopub.execute_input":"2024-04-12T15:01:48.094579Z","iopub.status.idle":"2024-04-12T15:01:50.106384Z","shell.execute_reply.started":"2024-04-12T15:01:48.094544Z","shell.execute_reply":"2024-04-12T15:01:50.105387Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0  'protests in paris ahead of putin visit to fre...   \n1    'donald trump gives $10,000 to pastor's family'   \n2  'california democrats propose in-state tuition...   \n3       'inner earth glows like in the movie avatar'   \n4  'clinton foundation ceo goes missing after tru...   \n\n                                                text  label  \n0  'paris (ap)  -   human rights activists are ga...      1  \n1  'chilling: what netanyahu is bracing for obama...      0  \n2  'california democrats have proposed a law to g...      1  \n3  'can there be light below the surface of the e...      0  \n4  ' another astonishing security council (sc) re...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>'protests in paris ahead of putin visit to fre...</td>\n      <td>'paris (ap)  -   human rights activists are ga...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>'donald trump gives $10,000 to pastor's family'</td>\n      <td>'chilling: what netanyahu is bracing for obama...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>'california democrats propose in-state tuition...</td>\n      <td>'california democrats have proposed a law to g...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>'inner earth glows like in the movie avatar'</td>\n      <td>'can there be light below the surface of the e...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>'clinton foundation ceo goes missing after tru...</td>\n      <td>' another astonishing security council (sc) re...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n\ntrain_df = train_data\ntrain_df[\"text\"] = tokenizer.cls_token + train_df['title'] + tokenizer.sep_token + train_df['text'] + tokenizer.sep_token\ntrain_df = train_df.drop(['title'], axis=1)\n\ndev_df = dev_data\ndev_df[\"text\"] = tokenizer.cls_token + dev_df['title'] + tokenizer.sep_token + dev_df['text'] + tokenizer.sep_token\ndev_df = dev_df.drop(['title'], axis=1)\n\ntest_df = test_data\ntest_df[\"text\"] = tokenizer.cls_token + test_df['title'] + tokenizer.sep_token + test_df['text'] + tokenizer.sep_token\ntest_df = test_df.drop(['title'], axis=1)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:01:51.998056Z","iopub.execute_input":"2024-04-12T15:01:51.998711Z","iopub.status.idle":"2024-04-12T15:01:54.199683Z","shell.execute_reply.started":"2024-04-12T15:01:51.998681Z","shell.execute_reply":"2024-04-12T15:01:54.198697Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966b2692ea3e4c269b2ffd5bc82c1cf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba962ce08334a0a8b9bba2d4b50be09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2c5faced9bc44058d1ec35a85b71aa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10d239bf5964527af6cd15349e018a3"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  [CLS]'protests in paris ahead of putin visit t...      1\n1  [CLS]'donald trump gives $10,000 to pastor's f...      0\n2  [CLS]'california democrats propose in-state tu...      1\n3  [CLS]'inner earth glows like in the movie avat...      0\n4  [CLS]'clinton foundation ceo goes missing afte...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS]'protests in paris ahead of putin visit t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS]'donald trump gives $10,000 to pastor's f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS]'california democrats propose in-state tu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS]'inner earth glows like in the movie avat...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS]'clinton foundation ceo goes missing afte...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['text']\n        label = self.data.iloc[idx]['label']\n\n        # Tokenize text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:01:55.961719Z","iopub.execute_input":"2024-04-12T15:01:55.962385Z","iopub.status.idle":"2024-04-12T15:01:55.970143Z","shell.execute_reply.started":"2024-04-12T15:01:55.962352Z","shell.execute_reply":"2024-04-12T15:01:55.969057Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_df, tokenizer, 512)\ndev_dataset = CustomDataset(dev_df, tokenizer, 512)\ntest_dataset = CustomDataset(test_df, tokenizer, 512)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:01:58.215599Z","iopub.execute_input":"2024-04-12T15:01:58.216495Z","iopub.status.idle":"2024-04-12T15:01:58.220880Z","shell.execute_reply.started":"2024-04-12T15:01:58.216463Z","shell.execute_reply":"2024-04-12T15:01:58.219795Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\nNow we can actually get into the training of our model. We will create a metrics function, set our arguments, and train our model before saving it so it can be used later without retraining.","metadata":{}},{"cell_type":"code","source":"def get_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    # Calculate metrics using scikit-learn\n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds, average='weighted')\n    recall = recall_score(labels, preds, average='weighted')\n    f1 = f1_score(labels, preds, average='weighted')\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:02:00.379847Z","iopub.execute_input":"2024-04-12T15:02:00.380570Z","iopub.status.idle":"2024-04-12T15:02:00.386095Z","shell.execute_reply.started":"2024-04-12T15:02:00.380541Z","shell.execute_reply":"2024-04-12T15:02:00.385107Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/models',          # output directory\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='/kaggle/working/logs',            # directory for storing\n    logging_steps=10,                # log training loss every n steps\n    evaluation_strategy=\"epoch\",     # evaluate model at the end of each epoch\n    save_strategy=\"epoch\",             # save model checkpoint at the end of each epoch\n    save_total_limit=3,              # Limit the total number of saved models\n    save_steps=500,\n)\n\ncredibility_trainer = Trainer(\n    model=model,                     # the instantiated 🤗 Transformers model to be trained\n    args=training_args,              # training arguments\n    train_dataset=train_dataset,     # training dataset\n    eval_dataset=dev_dataset,        # evaluation dataset\n    tokenizer=tokenizer,             # tokenizer for encoding input data\n    compute_metrics=get_metrics\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"credibility_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T00:52:35.110068Z","iopub.execute_input":"2024-04-12T00:52:35.110466Z","iopub.status.idle":"2024-04-12T01:43:00.398927Z","shell.execute_reply.started":"2024-04-12T00:52:35.110433Z","shell.execute_reply":"2024-04-12T01:43:00.398035Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2949' max='2949' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2949/2949 50:22, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.038100</td>\n      <td>0.029144</td>\n      <td>0.990336</td>\n      <td>0.990338</td>\n      <td>0.990336</td>\n      <td>0.990334</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.014000</td>\n      <td>0.020633</td>\n      <td>0.990844</td>\n      <td>0.990858</td>\n      <td>0.990844</td>\n      <td>0.990842</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.030100</td>\n      <td>0.030843</td>\n      <td>0.990844</td>\n      <td>0.990858</td>\n      <td>0.990844</td>\n      <td>0.990842</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2949, training_loss=0.05536111957614969, metrics={'train_runtime': 3024.9646, 'train_samples_per_second': 15.596, 'train_steps_per_second': 0.975, 'total_flos': 6249546933792768.0, 'train_loss': 0.05536111957614969, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"def zip_and_move_folder(source_folder, zip_name, destination_folder):\n    # Ensure source_folder exists\n    if not os.path.exists(source_folder):\n        print(f\"Error: Folder '{source_folder}' not found.\")\n        return\n\n    # Ensure destination_folder exists\n    if not os.path.exists(destination_folder):\n        print(f\"Error: Destination folder '{destination_folder}' not found.\")\n        return\n\n    # Ensure zip_name has a .zip extension\n    if not zip_name.endswith('.zip'):\n        zip_name += '.zip'\n\n    # Zip the source_folder\n    zip_path = os.path.join(destination_folder, zip_name)\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(source_folder):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zipf.write(file_path, os.path.relpath(file_path, source_folder))\n\n    # Move the zipped folder to destination_folder\n    shutil.move(zip_path, os.path.join(destination_folder, zip_name))\n\n    print(f\"Folder '{source_folder}' zipped as '{zip_name}' and moved to '{destination_folder}'.\")\n\nsource_folder = \"/kaggle/working/models/checkpoint-2949\"\nzip_name = \"model.zip\"\ndestination_folder = '/kaggle/working'\n\nzip_and_move_folder(source_folder, zip_name, destination_folder)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T02:01:42.393625Z","iopub.execute_input":"2024-04-12T02:01:42.394015Z","iopub.status.idle":"2024-04-12T02:02:24.266277Z","shell.execute_reply.started":"2024-04-12T02:01:42.393985Z","shell.execute_reply":"2024-04-12T02:02:24.265311Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Folder '/kaggle/working/models/checkpoint-2949' zipped as 'model.zip' and moved to '/kaggle/working'.\n","output_type":"stream"}]},{"cell_type":"code","source":"test_dataset = CustomDataset(test_df, tokenizer, 512)\n\nresults = credibility_trainer.evaluate(eval_dataset=test_dataset)\n\nresults_df = pd.DataFrame(results, index=[0])\nresults_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# free up memory for different tasks\ndel train_data\ndel test_data\ndel dev_data\ndel train_df\ndel train_dataset\ndel dev_df\ndel dev_dataset\ndel credibility_trainer","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:02:09.916521Z","iopub.execute_input":"2024-04-12T15:02:09.917265Z","iopub.status.idle":"2024-04-12T15:02:10.544155Z","shell.execute_reply.started":"2024-04-12T15:02:09.917235Z","shell.execute_reply":"2024-04-12T15:02:10.542801Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dev_df\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m dev_dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m credibility_trainer\n","\u001b[0;31mNameError\u001b[0m: name 'credibility_trainer' is not defined"],"ename":"NameError","evalue":"name 'credibility_trainer' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"### Loading and Predicting\nThis final section hows to load the model, make predictions, and save them.","metadata":{}},{"cell_type":"code","source":"# Load the trained weights from the checkpoint file\ncheckpoint_path = \"/kaggle/input/modelzip\"\nmodel = DistilBertForSequenceClassification.from_pretrained(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:02:15.924666Z","iopub.execute_input":"2024-04-12T15:02:15.925023Z","iopub.status.idle":"2024-04-12T15:02:17.753226Z","shell.execute_reply.started":"2024-04-12T15:02:15.924995Z","shell.execute_reply":"2024-04-12T15:02:17.752150Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"In this next part we show the predictions evaluated ont he test data of our newly loaded model, verifying that the saved and loaded model is the same as the previously trained model.","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Instantiate Trainer with the trained model and evaluation arguments\ntrainer = Trainer(\n    model=model,  # The trained model\n    tokenizer=tokenizer,  # The tokenizer associated with the model\n    compute_metrics=get_metrics,  # Function to compute evaluation metrics\n)\n\n# Evaluate the model\nresults = trainer.evaluate(test_dataset)\n\n# Print the evaluation results\nresults_df = pd.DataFrame(results, index=[0])\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-04-12T14:28:10.405994Z","iopub.execute_input":"2024-04-12T14:28:10.406437Z","iopub.status.idle":"2024-04-12T14:48:20.058681Z","shell.execute_reply.started":"2024-04-12T14:28:10.406403Z","shell.execute_reply":"2024-04-12T14:48:20.057361Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [246/246 20:04]\n    </div>\n    "},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   eval_loss  eval_accuracy  eval_precision  eval_recall  eval_f1  \\\n0   0.039523        0.99084        0.990843      0.99084  0.99084   \n\n   eval_runtime  eval_samples_per_second  eval_steps_per_second  \n0     1209.5748                    1.625                  0.203  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eval_loss</th>\n      <th>eval_accuracy</th>\n      <th>eval_precision</th>\n      <th>eval_recall</th>\n      <th>eval_f1</th>\n      <th>eval_runtime</th>\n      <th>eval_samples_per_second</th>\n      <th>eval_steps_per_second</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.039523</td>\n      <td>0.99084</td>\n      <td>0.990843</td>\n      <td>0.99084</td>\n      <td>0.99084</td>\n      <td>1209.5748</td>\n      <td>1.625</td>\n      <td>0.203</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_to_predict = test_df[\"text\"].tolist()\n\n#tokenized_data = tokenizer(, truncation=True, padding=True, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:09:25.747197Z","iopub.execute_input":"2024-04-12T15:09:25.747583Z","iopub.status.idle":"2024-04-12T15:09:25.752369Z","shell.execute_reply.started":"2024-04-12T15:09:25.747556Z","shell.execute_reply":"2024-04-12T15:09:25.751243Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#torch.cuda.empty_cache()\n# tokenized_data_gpu = {key: val.to('cuda') for key, val in tokenized_data.items()}\nmodel = model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:09:28.463907Z","iopub.execute_input":"2024-04-12T15:09:28.464921Z","iopub.status.idle":"2024-04-12T15:09:28.471043Z","shell.execute_reply.started":"2024-04-12T15:09:28.464888Z","shell.execute_reply":"2024-04-12T15:09:28.470212Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"probs = []\nbatch_size = 4\nfor i in range(0, len(data_to_predict), batch_size):\n    batch_data = data_to_predict[i:i+batch_size]\n\n    # Tokenize batch_data here using your tokenizer\n    tokenized_data = tokenizer(batch_data, truncation=True, padding=True, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        # Process tokenized_data here\n        outputs = model(**tokenized_data.to('cuda'))  # Assuming model is on the appropriate device\n        logits = outputs.logits\n        probabilities = torch.softmax(logits, dim=1)\n        \n        probs.append(probabilities)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:10:53.721189Z","iopub.execute_input":"2024-04-12T15:10:53.721563Z","iopub.status.idle":"2024-04-12T15:11:57.561650Z","shell.execute_reply.started":"2024-04-12T15:10:53.721533Z","shell.execute_reply":"2024-04-12T15:11:57.560796Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"combined_probabilities = []\n\n# Iterate through probs and concatenate every batch_size tensors\nfor i in range(0, len(probs), batch_size):\n    # Extract a batch of tensors\n    batch_tensors = probs[i:i + batch_size]\n    \n    # Concatenate the batch tensors along dim=0 (assuming they have the same shape)\n    concatenated_tensor = torch.cat(batch_tensors, dim=0)\n    \n    # Append the concatenated tensor to the combined list\n    combined_probabilities.append(concatenated_tensor)\n\nclass1_probs = []\n\n# Process each batch\nfor probs_batch in combined_probabilities:\n    # Extract probabilities of class 1 (index 1)\n    class1_probs_batch = probs_batch[:, 1]  # Assuming class 1 is in the second column\n    class1_probs.extend(class1_probs_batch.tolist())  # Convert to list and extend the main list","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:19:21.338714Z","iopub.execute_input":"2024-04-12T15:19:21.339506Z","iopub.status.idle":"2024-04-12T15:19:21.355633Z","shell.execute_reply.started":"2024-04-12T15:19:21.339472Z","shell.execute_reply":"2024-04-12T15:19:21.354779Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"test_df['predicted_prob'] = class1_probs\ntest_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T15:21:33.201253Z","iopub.execute_input":"2024-04-12T15:21:33.201951Z","iopub.status.idle":"2024-04-12T15:21:33.216138Z","shell.execute_reply.started":"2024-04-12T15:21:33.201918Z","shell.execute_reply":"2024-04-12T15:21:33.215200Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                                text  label  predicted_prob\n0  [CLS]'why the truth might get you fired'[SEP]'...      0        0.000110\n1  [CLS]'monica lewinsky, clinton sex scandal set...      1        0.999976\n2  [CLS]'humiliated hillary tries to hide what ca...      0        0.000110\n3  [CLS]'study: more than half of car crashes inv...      1        0.999977\n4  [CLS]'mindful eating as way to fight bingeing ...      1        0.999965\n5  [CLS]'massive anti-trump protests, union squar...      0        0.000117\n6  [CLS]'turkey threatens to open migrant 'land p...      1        0.999975\n7  [CLS]'mike birbiglia's 6 tips for making it sm...      1        0.999977\n8  [CLS]''chapo trap house': new left-wing podcas...      0        0.000106\n9  [CLS]'the beautiful prehistoric world: is eart...      0        0.000125","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>predicted_prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[CLS]'why the truth might get you fired'[SEP]'...</td>\n      <td>0</td>\n      <td>0.000110</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[CLS]'monica lewinsky, clinton sex scandal set...</td>\n      <td>1</td>\n      <td>0.999976</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[CLS]'humiliated hillary tries to hide what ca...</td>\n      <td>0</td>\n      <td>0.000110</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[CLS]'study: more than half of car crashes inv...</td>\n      <td>1</td>\n      <td>0.999977</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[CLS]'mindful eating as way to fight bingeing ...</td>\n      <td>1</td>\n      <td>0.999965</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>[CLS]'massive anti-trump protests, union squar...</td>\n      <td>0</td>\n      <td>0.000117</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>[CLS]'turkey threatens to open migrant 'land p...</td>\n      <td>1</td>\n      <td>0.999975</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>[CLS]'mike birbiglia's 6 tips for making it sm...</td>\n      <td>1</td>\n      <td>0.999977</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>[CLS]''chapo trap house': new left-wing podcas...</td>\n      <td>0</td>\n      <td>0.000106</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>[CLS]'the beautiful prehistoric world: is eart...</td>\n      <td>0</td>\n      <td>0.000125</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}